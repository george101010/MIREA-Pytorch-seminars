{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGV9hCmFpCxUpCgB6bMET1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "OIxaMb2eDN16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Решение домашнего задания для семинара 28.11.22."
      ],
      "metadata": {
        "id": "GMsVzAn3ChQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дедлайн домашнего задания - текущее воскресенье 23:59. "
      ],
      "metadata": {
        "id": "3hbaklz_96EN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Домашнее задание 0:** Создайте репозиторий на Github. Вся дальнейшая домашка загружается туда. Чуть позже в телеграм чате будет выложена форма для сдачи."
      ],
      "metadata": {
        "id": "E7VzrzlHyyci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Домашнее задание 1:** реализуйте XOR с помощью 3 нейронов. Запишите ответ в виде выражения, состоящего из объектов neuron() – моделей нейрона с пороговой функцией активации, внутри скобок может быть что угодно. Входы верхнего уровня называются x1 и x2. Пример фрагмента записи: neuron(1*x1 + 5*x2 - 0.1) + neuron(x1) (ответ будет выглядеть чуть сложнее, но других символов вроде && не потребуется)."
      ],
      "metadata": {
        "id": "me6riR-YEuYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Логический элемент XOR реализуется при помощи сети следующей структуры:\n",
        "* Входной слой: 2 нейрона\n",
        "* Скрытый слой 1: 2 нейрона\n",
        "* Выходной слой: 1 нейрон\n",
        "\n",
        "Таблица истинности функции XOR.\n",
        "\n",
        "| X1 | X2 | X1 XOR X2 |\n",
        "|----|----|-----------|\n",
        "| 0  | 0  | 0         |\n",
        "| 0  | 1  | 1         |\n",
        "| 1  | 0  | 1         |\n",
        "| 1  | 1  | 0         |\n",
        "\n",
        "С учетом нотации, описанной выше модель сети будет состоять из 3 объектов neuron():\n",
        "\n",
        "neuron(1 *neuron(1*x1-1*x2-0.5)+1*neuron(-1*x1+1*x2-0.5)-0.5)"
      ],
      "metadata": {
        "id": "xowQfM2MD9AI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Домашнее задание 2:** нарисуйте backward граф для выражения `a*b+c*d`. [Теория и пример оформления](https://www.youtube.com/watch?v=MswxJw-8PvE). Сравните полученные теоретические значения с аттрибутами grad у исходных тензоров."
      ],
      "metadata": {
        "id": "UQmVNc2kEyQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HNbIuSKD7lv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нарисованный граф находится в одной папке с данным ноутбуком"
      ],
      "metadata": {
        "id": "WN7MCzTW7ly2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([2.0], requires_grad=True)\n",
        "b = torch.tensor([4.0], requires_grad=True)\n",
        "c = torch.tensor([1.0], requires_grad=True)\n",
        "d = torch.tensor([5.0], requires_grad=False)"
      ],
      "metadata": {
        "id": "rHAT-rNxnh-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for NAME , VAR in zip( ['a','b','c','d'] , [a,b,c,d]):\n",
        "  print( NAME ,'grad: ', a.grad)"
      ],
      "metadata": {
        "id": "yQUF6a8rnjyF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0445114-816e-4cea-a1e5-d396518d1dbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a grad:  None\n",
            "b grad:  None\n",
            "c grad:  None\n",
            "d grad:  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = a*b + c*d\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "WQewq5DCnnPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for NAME , VAR in zip( ['a','b','c','d'] , [a,b,c,d]):\n",
        "  print( NAME ,'grad: ', a.grad)"
      ],
      "metadata": {
        "id": "tDxWZCwQnojN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b46eed10-5ee4-4379-cb97-6b0e7705f02f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a grad:  tensor([4.])\n",
            "b grad:  tensor([4.])\n",
            "c grad:  tensor([4.])\n",
            "d grad:  tensor([4.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теоретические значения градиентов у тензоров:\n",
        "* A -  4\n",
        "* B -  2\n",
        "* C -  5\n",
        "* D -  1"
      ],
      "metadata": {
        "id": "V5CpShWKdBeT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Домашнее задание 3:** Поэксперементируйте с размером тензоров, которые влезут на видеоркарту в Colab. Найдите максимальный размер тензора для типа данных float32, float64, float16, int32, int64. На сколько они отличаются."
      ],
      "metadata": {
        "id": "NxKEOwWDwv3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теоретические расчеты. \n",
        "\n",
        "Нужно запроить свободную память на CUDA, командой mem_get_info()\n",
        "\n",
        "Первое значения кортежа - количество свободной памяти.\n",
        "\n",
        "Есть переменные на 2,4,8 байт.\n",
        "\n",
        "Тогда можно поделить свободное пространство на эти числа - полученный результат - это количество элементов в массиве. Для простоты можно записать 1 мерный массив.\n",
        "\n",
        "От полученных чисел можно оттолкнуться в экспериментах"
      ],
      "metadata": {
        "id": "MXq81Ll-IV1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# подключение CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3ME_cEZ-UXq",
        "outputId": "45ea2a0a-e458-4876-ebe0-c3f933e848e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwfKP84P-2JB",
        "outputId": "67d67e3d-6a47-48c2-fb6e-14d2c4da9b89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_properties(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1x9g74T-MEk",
        "outputId": "06495669-0920-49f5-95ef-f74000693b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15109MB, multi_processor_count=40)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Эксперимент с тензором типа float32 \n",
        "\n",
        "Тензор, который влез в память.\n",
        " "
      ],
      "metadata": {
        "id": "v_ht_kPvA3H-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "5HfmeM6PKLd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.zeros(39000000 , 1,dtype=torch.float32, device='cuda')\n",
        "print('Занято: ',torch.cuda.memory_allocated() ,' байт')\n",
        " \n"
      ],
      "metadata": {
        "id": "-wvqpwY0AqQ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cf9ee45-7dd3-4cc1-ae35-7f0b071d6c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Занято:  156000256  байт\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Эксперимент с тензором типа float64\n",
        "\n",
        "Сверху тензор, который влез в память.\n",
        "Снизу тензор чуть больше, который уже не влез. "
      ],
      "metadata": {
        "id": "leWH-9WSBr0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.zeros(100000  * 19000 ,dtype=torch.float64,device='cuda')"
      ],
      "metadata": {
        "id": "tZt9SQiPBr0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.zeros(100000 * 20000 ,dtype=torch.float64,device='cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "9b7db750-037d-4ab7-875d-3c1e1956f050",
        "id": "YI-KIKhUBr0Q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f8f23fee9eaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m20000\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 14.90 GiB (GPU 0; 14.76 GiB total capacity; 0 bytes already allocated; 14.66 GiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_allocated()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4293641a-c80b-4cd9-d2f9-d0a7f250a47b",
        "id": "ZANs_C5PBr0R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15200157696"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-bjwzs1wDZnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Эксперимент с тензором типа float16\n",
        "\n",
        "Сверху тензор, который влез в память.\n",
        "Снизу тензор чуть больше, который уже не влез. "
      ],
      "metadata": {
        "id": "sY45fdb3DaPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.zeros(100000 * 75000 ,dtype=torch.float16,device='cuda')"
      ],
      "metadata": {
        "id": "8kVjiFPmDaPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.zeros(100000 * 80000 ,dtype=torch.float16,device='cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "5151fc0b-2919-4112-c227-83ccd7b24728",
        "id": "DkIwBCbuDaPs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-bc72e8f4aba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m80000\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 14.90 GiB (GPU 0; 14.76 GiB total capacity; 0 bytes already allocated; 14.66 GiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_allocated()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35fcf2fe-bb5a-42ec-da5a-5808f8d49d9d",
        "id": "dLNMSisoDaPs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15000928256"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ERgDp5zfEpAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Эксперимент с тензором типа int32\n",
        "\n",
        "Сверху тензор, который влез в память.\n",
        "Снизу тензор чуть больше, который уже не влез. "
      ],
      "metadata": {
        "id": "JNsjN2JhEpLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "3UuJjjfxFEpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.zeros(491907973*  7 ,dtype=torch.int32,device='cuda')"
      ],
      "metadata": {
        "id": "MEXOAenmEpLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.zeros(112241*  32300 ,dtype=torch.int32,device='cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "be0028e2-649c-4c81-c042-07493591b765",
        "id": "xrNxmyLtEpLN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a561c732b0c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m112241\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m36300\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 15.18 GiB (GPU 0; 14.76 GiB total capacity; 0 bytes already allocated; 14.66 GiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_allocated()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d976d3a1-2d3b-4857-ea1f-bdce072fc1a7",
        "id": "21sYFMbIEpLO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13774094336"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GOxePfhsFlf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Эксперимент с тензором типа int64\n",
        "\n",
        "Сверху тензор, который влез в память.\n",
        "Снизу тензор чуть больше, который уже не влез. "
      ],
      "metadata": {
        "id": "F-rm3KQmFlpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "z3L03p9kFlpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.zeros(100000 *  19000 ,dtype=torch.int64,device='cuda')"
      ],
      "metadata": {
        "id": "Kaffb-heGGzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.zeros(100000 * 20000 ,dtype=torch.int64,device='cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "971c14cd-9d09-42ca-b7b1-c73bb19bde2b",
        "id": "w_PqgO5PFlpU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5ee91aa2ac3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m20000\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 14.90 GiB (GPU 0; 14.76 GiB total capacity; 0 bytes already allocated; 14.66 GiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_allocated()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c649190-3991-4da9-f01d-1b9bc3111c25",
        "id": "b4sH6jRpFlpV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15200157696"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Домашнее задание 4:** Напишите хороший пример неэффективного кода для занятия памяти видеокарты, который вызовет ошибку out of memory"
      ],
      "metadata": {
        "id": "CBR5pWOlzOtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Два вектора размером 100 000 элементов каждый нужно было перемножить, чтобы получить матрицу. Один вектор транспонирован. Оба вектора хранились в памяти CUDA. Но полученная матрица не поместилась в память."
      ],
      "metadata": {
        "id": "9qVw8DWWs59D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "e-vrx9U8vsZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# память не занята\n",
        "torch.cuda.memory_allocated()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zP_1CAlmtDfR",
        "outputId": "51471d34-946b-490e-f09e-f1fcab260e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# векторы с целыми случайными числами\n",
        "# A - вектор столбец\n",
        "# B - вектор строка\n",
        "A = torch.randint(low=3, high = 1000, size = (100000,1) , device = torch.device('cuda'))\n",
        "B = torch.randint(low=3, high = 1000, size = ( 1,100000 ) , device = torch.device('cuda'))"
      ],
      "metadata": {
        "id": "qCWXC0E9tE9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_allocated()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wWXxfV4tJPK",
        "outputId": "46dbd178-3684-4f87-b5f5-988543737fbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1600512"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# С - матрица - C = A * B\n",
        "C = torch.matmul(input =  A , other =  B ).to( torch.device('cuda') )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "CCysvzIzr0eM",
        "outputId": "5ea7aa76-8baf-454a-adc4-df5dbe543601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-18bc217c9137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mA\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mB\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 74.51 GiB (GPU 0; 14.76 GiB total capacity; 1.53 MiB already allocated; 14.16 GiB free; 2.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Матрица не поместилась в память"
      ],
      "metadata": {
        "id": "RHhSPHYOxBIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Домашнее задание 5:** Используя один линейный слой `nn.Linear` и один входной тензор `x` подберите подберите размерности так, чтобы занимать всю видеопамять.\n",
        "Попробуйте применить линейный слой к тензору `x`. Что произойдет? Кратко опишите ваши эксперименты. Что вы поняли?"
      ],
      "metadata": {
        "id": "0_hOtvvR89jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "На вход поступает тензор \n",
        "* размер батча = 200 000\n",
        "* размерность вектора признаков = 19 000\n",
        "\n",
        "Есть линейный слой с входом на 19000 нейронов и 1 выходным нейроном\n",
        "\n",
        "На каждом шагу демонстрируется занятая тензорами память"
      ],
      "metadata": {
        "id": "koSpPvpIyeOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# подключение CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpmgl64pzb9_",
        "outputId": "cc0f792a-c12d-4cd6-b975-542a490f466c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size =  200000  \n",
        "n_input = 19000\n",
        "data_x = torch.randn(batch_size, n_input , device =device )"
      ],
      "metadata": {
        "id": "qR9khi1Fy_kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# занято памяти\n",
        "torch.cuda.memory_allocated()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thOf5L0xzNfI",
        "outputId": "324d7f90-f3ef-4fed-c5e2-e3bfbeb9616f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15200157696"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# линейный слой\n",
        "L = torch.nn.Linear(  19000, 1, device= device)\n"
      ],
      "metadata": {
        "id": "C8m0CHKBxIfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# веса и смещение слоя\n",
        "L.weight, L.bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P86mke0o5QQL",
        "outputId": "d56ea055-48f1-4c5f-ec0d-7f5916ad2d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Parameter containing:\n",
              " tensor([[-0.0068,  0.0067, -0.0013,  ..., -0.0043,  0.0023, -0.0040]],\n",
              "        device='cuda:0', requires_grad=True), Parameter containing:\n",
              " tensor([0.0049], device='cuda:0', requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# памяти занято чуть больше\n",
        "torch.cuda.memory_allocated()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD6RBtoe5O3d",
        "outputId": "01f75205-0c80-42da-a0eb-e590eb99553e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15200234496"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# передача тензора в слой для вычислений\n",
        "out = L(data_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "3jW2KeHx17wb",
        "outputId": "c5872cc3-6aaa-47f1-d4a2-3c9759eec81e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5fb78641104d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CUBLAS - это библиотека алгоритмов для CUDA. Как показывает изучение ошибки - эта ошибка вызвана нехваткой памяти для вычислений выхода слоя. "
      ],
      "metadata": {
        "id": "LvCze3qp6YrU"
      }
    }
  ]
}